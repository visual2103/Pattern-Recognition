{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7964da77-8746-458d-b1c7-ae8655443bb6",
   "metadata": {},
   "source": [
    "# **Laborator 8: Rețele neuronale**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2695461-1fcc-4c7a-95d1-68f216478635",
   "metadata": {},
   "source": [
    "În această lucrare de laborator se prezintă rețelele neuronale si se va implementa un tip de rețea numit **perceptron cu mai multe straturi (MLP).**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1f80882-4fb7-4d66-8a57-ad2354d1ab96",
   "metadata": {},
   "source": [
    "O rețea neuronală este un model computațional non-linear inspirat de creierul uman, conceput pentru a recunoaște tipare și a învăța din date. În forma sa cea mai simplă, o rețea neuronală constă din „neuroni” interconectați, organizați în straturi. Fiecare neuron procesează intrări pentru a produce o ieșire, pe care o transmite următorului strat, permițând astfel rețelei să învețe reprezentări complexe. \n",
    "\n",
    "Componentele unei rețele neuronale sunt:\n",
    "\n",
    "1. **Neuron (Perceptron)**: Elementul fundamental al unei rețele neuronale care calculează o combinație liniară a trăsăturilor de intrare pe baza ponderilor $w$ si a deplasamentului $b$:\n",
    "$$\n",
    "  z = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "2. **Funcția de Activare**: Adaugă non-liniaritate, permițând rețelelor neuronale să învețe relații complexe. Funcțiile cele mai utilizate sunt:\n",
    "\n",
    "\n",
    "- ReLU (Rectified Linear Unit)\n",
    "$$ f(z) = max(z, 0)$$\n",
    "\n",
    "- Sigmoid: Această funcție primește un număr real și îl comprimă într-un interval între 0 și 1, astfel numerele negative mari devin 0, iar numerele pozitive mari devin 1. Se poate folosi de asemenea în clasificare binară pentru a transforma scoruri în probabilități.\n",
    "\n",
    "$$ f(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/sigmoid.png\" alt=\"image\" width=\"200\" title=\"Neuron\"/>\n",
    "</p> \n",
    "<p align=\"center\"><em>Figura 1. Funcția sigmoid. </em></p>\n",
    "\n",
    "- Softmax (pentru clasificare multi-clasă): Această funcție este folosită cel mai des pentru a transforma scorurile corespunzătoare claselor într-o distribuție de probabilitate.\n",
    "\n",
    "$$ f(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_i}} $$\n",
    "\n",
    "unde K reprezintă numărul de clase, iar $z_i$ este scorul pentru clasa $i$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/neuron.png\" alt=\"image\" width=\"300\" title=\"Neuron\"/>\n",
    "</p> \n",
    "<p align=\"center\"><em>Figura 2. Modelul matematic al unui neuron. </em></p>\n",
    "\n",
    "\n",
    "Rețelele neuronale sunt modelate ca grupuri de neuroni conectați într-un graf aciclic. Cu alte cuvinte, ieșirile unor neuroni pot deveni intrări pentru alți neuroni. Modelele de rețele neuronale sunt adesea organizate în straturi de neuroni. Pentru rețelele neuronale standard, cel mai comun tip de strat este **stratul complet conectat**, în care toți neuronii dintre două straturi adiacente sunt conectați între ei,\n",
    "iar neuronii dintr-un singur strat nu au conexiuni între ei. Definim trei tipuri principale de straturi:\n",
    "\n",
    "- **Stratul de Intrare**: Primește datele brute, fiecare nod reprezentând o trăsătură de intrare.\n",
    "- **Straturi Ascunse**: Straturi intermediare care învață să extragă trăsături mai complexe.\n",
    "- **Stratul de Ieșire**: Oferă predicția finală, organizată diferit în funcție de sarcină (de exemplu, un nod pentru clasificare binară sau mai multe noduri pentru clasificare multi-clasă).\n",
    "\n",
    "\n",
    "Mai jos sunt prezentate două exemple de topologii de rețele neuronale care folosesc straturi complet conectate:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/mlp.png\" alt=\"image\" width=\"600\" title=\"Neuron\"/>\n",
    "</p> \n",
    "<p align=\"center\"><em>Figura 3. Stânga, o rețea cu 2 straturi (un strat ascuns și un strat de ieșire). Dreapta, o rețea cu 3 straturi (două straturi ascunse și un strat de ieșire). Aceste rețele folosesc straturi complet conectate, unde fiecare neuron este conectat cu fiecare neuron din stratul următor. </em></p>\n",
    "\n",
    "\n",
    "### Perceptronii cu Mai Multe Straturi (MLP)\n",
    "Un perceptron cu mai multe straturi (MLP) este un tip de rețea neuronală format dintr-un strat de intrare, unul sau mai multe straturi ascunse și un strat de ieșire. Fiecare strat din MLP este complet conectat, adică fiecare neuron dintr-un strat este conectat la fiecare neuron din stratul următor.\n",
    "\n",
    "#### Utilizarea MLP-urilor pentru Clasificare\n",
    "\n",
    "##### **1. Concepte generale**\n",
    "În clasificare, scopul MLP-ului este de a împărți datele în clase distincte. În cazul clasificării binare, stratul de ieșire va avea un singur neuron care folosește funcția de activare sigmoid. Astfel, ieșirea rețelei reprezintă probilitatea clasei 1:\n",
    "\n",
    "$$\n",
    "p(y = 1 | \\mathbf{x}; \\boldsymbol{\\theta}) = \\sigma(a) = \\frac{1}{1 + e^{-a}}\n",
    "$$\n",
    "\n",
    "unde $y$ reprezintă clasa, $x$ sunt datele de intrare ale retelei, $\\theta$ sunt parametrii modelului, $\\sigma$ reprezintă funcția sigmoid și $a$ reprezintă **logit-urile**, adică ieșirea ultimului neuron  înaintea aplicării funcției de activare sigmoid. \n",
    "\n",
    "În cazul clasificării multi-clasă, stratul de ieșire al rețelei va avea un neuron per clasă care folosește funcția de activare softmax. \n",
    "\n",
    "Un perceptron cu mai multe straturi (MLP) încearcă să găsească o suprafață de decizie non-lineară optimă folosind datele de *antrenare*. Astfel, prin procesul de antrenare, modelul își ajustează parametrii (ponderile și deplasamentele), pentru a învăța să separe corect datele de antrenare în clasele corespunzătoare. Scopul este de a obține un model care să poată clasifica datele noi, dintr-un set de test, cu o acuratețe cât mai mare.\n",
    "\n",
    "\n",
    "##### **2. Împărțirea setului de date**\n",
    "Împărțirea setului de date în subseturi de antrenare, validare și test este o practică esențială în antrenarea rețelelor neuronale.\n",
    "\n",
    "1. Setul de Antrenare: Modelul folosește acest subset pentru a-și ajusta parametrii (ponderile și deplasamentul) în timpul antrenării. De obicei, reprezintă cea mai mare parte a datelor, în jur de 60-70% din totalul setului de date.\n",
    "\n",
    "2. Setul de Validare: Acest subset este folosit pentru a evalua performanța modelului în timpul antrenării și pentru a ajusta hiperparametrii (cum ar fi numărul de straturi, numărul de neuroni etc.). Setul de validare nu influențează direct ajustarea parametrilor modelului, ci oferă feedback pentru a preveni supraantrenarea (overfitting) și pentru a optimiza modelul. De obicei, este în jur de 10-20% din setul de date.\n",
    "\n",
    "3. Setul de Test: După ce modelul este antrenat și optimizat folosind setul de validare, setul de test este utilizat pentru a evalua performanța finală a modelului. Scopul acestui subset este de a oferi o estimare a capacității modelului de a generaliza pe date noi și necunoscute. De obicei, este în jur de 10-20% din setul de date.\n",
    "\n",
    "\n",
    "##### **3. Antrenarea MLP**\n",
    "\n",
    "Pentru a găsi parametrii (ponderile și deplasamentul) optimi ai modelului în timpul antrenării, se definește o funcție de pierdere care măsoară cât de bine corespund predicțiile rețelei cu etichetele reale. \n",
    "\n",
    "Astfel în cazul clasificării binare, funcția de pierdere folosită este **binary cross-entropy**:\n",
    "$$\n",
    "\\text{BCELoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right)\n",
    "$$\n",
    "\n",
    "- unde N reprezintă numărul total de exemple\n",
    "- $y_i$ este eticheta reală (0 sau 1)\n",
    "- $\\hat y_i$ este probabilitatea prezisă de model pentru clasa pozitivă (1).\n",
    "\n",
    "Pentru a minimiza funcția de pierdere se folosește de obicei un proces de optimizare iterativ, precum **gradient descent**. Acest algoritm realizează iterativ o actualizare a parametrilor în direcția inversă gradientului pentru a asigura scăderea funcției de pierdere.\n",
    "\n",
    "Actualizarea parametrilor se realizează astfel:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} \\text{BCELoss}\n",
    "$$\n",
    "\n",
    "- unde α este rata de învățare\n",
    "- $\\nabla_{\\theta} \\text{BCELoss}$ este gradientul funcției de pierdere față de parametrii și conține derivatele parțiale ale funcției de pierdere față de fiecare parametru al modelului\n",
    "\n",
    "Gradient descent poate fi aplicat în trei moduri principale: **batch gradient descent**, **stochastic gradient descent (SGD)** și **mini-batch gradient descent**. \n",
    "\n",
    "1. Batch gradient descent: Gradientul este calculat folosind întregul set de antrenare la fiecare pas de actualizare.\n",
    "2. Stochastic gradient descent: Gradientul este calculat și parametrii sunt actualizați pentru fiecare exemplu din setul de antrenare.\n",
    "3. Mini-Batch Gradient Descent: Setul de date este împărțit în subseturi mici (mini-batch-uri), iar gradientul este calculat și parametrii sunt actualizați pentru fiecare mini-batch.\n",
    "\n",
    "Pașii antrenării unui MLP sunt: \n",
    "\n",
    "1. **Propagarea înainte** (Forward pass): Datele de intrare sunt transmise prin rețea strat cu strat, de la stratul de intrare la cel de ieșire. Fiecare neuron calculează o combinație liniară a intrărilor (suma ponderată a intrărilor plus un termen de deplasament), aplică o funcție de activare și transmite ieșirea mai departe. Această fază produce predicția rețelei pentru un anumit set de intrări.\n",
    "\n",
    "2. **Calculul erorii** (Funcția de Pierdere): După ce se obține predicția, aceasta este comparată cu valoarea reală (eticheta adevărată). \n",
    "\n",
    "3. **Propagarea înapoi** (Backpropagation): În această fază, se calculează derivatelor parțiale ale funcției de pierdere față de fiecare parametru al modelului prin propagarea derivatelor de la stratul de ieșire spre straturile ascunse folosind regula lanțului.\n",
    "\n",
    "6. **Actualizarea ponderilor**: Ponderile sunt ajustate în sensul descendent al gradientului utilizând metoda gradient descent.\n",
    "\n",
    "##### **4. Testarea MLP**\n",
    "\n",
    "În faza de testare a unui MLP, scopul este de a evalua performanța acestuia pe date noi, necunoscute, care nu au fost folosite în antrenare sau validare. În această etapă, modelul realizează doar propagarea înainte (forward pass) pentru a face predicții, fără a mai modifica parametrii. \n",
    "\n",
    "În cazul clasificării binare, probabilitatea obținută $p(y=1|x; \\theta)$ se va compara cu un prag pentru a găsi clasa prezisă:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\text{if } p(y=1|x; \\theta) < 0.5 \\text{ predict class 0} \\\\\n",
    "\\text{if } p(y=1|x; \\theta) \\geq 0.5 \\text{ predict class 1}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "În exercițiile din acest laborator, veți folosi o rețea neuronală pentru a recunoaște două cifre scrise de mână, 0 și 1, din imagini ale setului de date MNIST. Aceasta este o sarcină de clasificare binară."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7706f07-6c35-4e46-89cd-1e71457c6337",
   "metadata": {},
   "source": [
    "**Ex1. Citirea și vizualizarea datelor.**\n",
    "\n",
    "Setul de antrenare conține 12665 de exemple cu cifre scrise de mână, zero și unu, stocate în fișierul X_train.npy.\n",
    "\n",
    "Fiecare exemplu de antrenare este o imagine în tonuri de gri de 28 x 28 pixeli a cifrei respective. Matricea de pixeli 28 x 28 este desfășurată într-un vector de dimensiune 784. Astfel, fiecare exemplu de antrenare devine un rând în matricea de trăsături X. Avem o matrice X de dimensiune 12665 x 784, în care fiecare rând este un exemplu de antrenare al unei imagini cu o cifră scrisă de mână.\n",
    "$$\n",
    "X = \n",
    "\\left(\\begin{array}{cc} \n",
    "--- (x^{(1)}) --- \\\\\n",
    "--- (x^{(2)}) --- \\\\\n",
    "\\vdots \\\\ \n",
    "--- (x^{(m)}) --- \n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "În fișierul Y_train.npy sunt stocate etichetele corespunzătoare fiecare exemplu de antrenare:\n",
    "- dacă y = 0 imaginea reprezintă cifra 0\n",
    "- dacă y = 1 imaginea reprezintă cifra 1\n",
    "\n",
    "De asemenea, în fișierele X_test.npy si Y_test.npy se află datele de test în același format.\n",
    "\n",
    "Se vor citi datele de antrenare si test și se vor vizualiza imaginile astfel:\n",
    "\n",
    "- se selectează aleator 5 imagini, se formează imaginile de dimensiune 28x28 și se afișează într-un grid\n",
    "- eticheta fiecărei imagini se va afișa deasupra imaginii\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "753d5eca-dec1-4cbf-b88c-8b005397972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12665, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = np.load('MNIST/X_train.npy')\n",
    "X_test = np.load('MNIST/X_test.npy')\n",
    "y_train = np.load('MNIST/Y_train.npy')\n",
    "y_test = np.load('MNIST/Y_test.npy')\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "fig_train, ax_train = plt.subplots(figsize=(16, 8), nrows=1, ncols=5)\n",
    "fig_train.suptitle(\"TRAIN set\", y=0.73, fontsize=16, fontweight='bold')\n",
    "\n",
    "fig_test, ax_test = plt.subplots(figsize=(16, 8), nrows=1, ncols=5)\n",
    "fig_test.suptitle(\"TEST set\", y=0.73, fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx in range(5):\n",
    "    i, j = np.random.randint(X_train.shape[0]), np.random.randint(X_test.shape[0])\n",
    "    \n",
    "    ax_train[idx].imshow(X_train[i].reshape(28, 28), cmap = matplotlib.cm.binary)\n",
    "    ax_train[idx].set_title(str(y_train[i]))\n",
    "    ax_train[idx].axis('off')\n",
    "    \n",
    "    ax_test[idx].imshow(X_test[j].reshape(28, 28), cmap = matplotlib.cm.binary)\n",
    "    ax_test[idx].set_title(str(y_test[j]))\n",
    "    ax_test[idx].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2dbbe-7c1d-4529-9699-7b68bea43ae1",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "Rețeaua neuronală are trei straturi dense cu funcții de activare ReLU în straturile ascunse și funcția de activare sigmoid în ultimul strat.\n",
    "\n",
    "- Intrările sunt valorile pixelilor din imaginile cu cifre. Deoarece imaginile au dimensiunea de 28 x 28 pixeli, rezultă 784 de intrări.\n",
    "- Rețeaua neuronală are 25 de neuroni în stratul 1, 15 neuroni în stratul 2 și 1 neuron în stratul 3.\n",
    "\n",
    "Dimensiunile parametrilor rețelei cu straturi dense sunt determinate astfel:\n",
    "\n",
    "Dacă rețeaua are $n$ neuroni într-un strat și $m$ neuroni în stratul următor, atunci ponderile $W$  vor avea dimensiunea $(n, m)$, iar deplasamentul $b$ va fi un vector cu $m$ elemente.\n",
    "\n",
    "Astfel, dimensiunile matricilor $W$ și vectorilor $b$ sunt:\n",
    "\n",
    "- **Stratul 1**: Dimensiunea lui $W_1$ este (784, 25), iar dimensiunea lui $b_1$ este (1, 25).\n",
    "- **Stratul 2**: Dimensiunea lui $W_2$ este (25, 15), iar dimensiunea lui $b_2$ este (1, 15).\n",
    "- **Stratul 3**: Dimensiunea lui $W_3$  este (15, 1), iar dimensiunea lui $b_3$ este (1, 1).\n",
    "\n",
    "Pentru un exemplu de antrenare, fiecare neuron calculează produsul scalar între vectorul de intrare și o coloană din vectorul de ponderi, după care aplică funcția de activare. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/mlp_architecture.png\" alt=\"image\" width=\"400\" title=\"Neuron\"/>\n",
    "</p> \n",
    "<p align=\"center\"><em>Figura 4. Rețea neuronală. </em></p>\n",
    "\n",
    "**Inițializarea parametrilor**\n",
    "\n",
    "1. Inițializarea deplasamentului (b) se face de obicei cu 0 sau cu o valoare constantă mică, de exemplu 0.01.\n",
    "\n",
    "\n",
    "2. Inițializarea ponderilor se poate cu numere aleatorii extrase dintr-o distribuție Gaussiană. \n",
    "\n",
    "$$\n",
    "w \\sim \\mathcal{N}\\left(0, 1 \\right)\n",
    "$$\n",
    "\n",
    "Astfel se face presupunerea că neuronii din toate straturile vor funcționa bine cu valori extrase din aceeași distribuție, indiferent de dimensiunea lor. În practică acest lucru poate ridica probleme din următoarele motive:\n",
    "\n",
    "- Ponderi prea mici: Gradienții se micșorează pe măsură ce se propagă prin straturi, devenind în cele din urmă atât de mici încât rețeaua nu mai învață. Acest fenomen este cunoscut sub numele de problema gradienților care dispar (vanishing gradients).\n",
    "\n",
    "- Ponderi prea mari: Gradienții cresc necontrolat pe măsură ce se propagă prin straturi, ducând la problema gradienților care explodează (exploding gradients).\n",
    "\n",
    "O soluție la această problemă reprezintă inițializarea He [1], care se folosește mai ales în contextul utilizării funcției de activare ReLU. Astfel, ponderile se extrag dintr-o distribuție normală cu varianța scalată de numărul de intrări ale neuronului $n_{in}$.\n",
    "\n",
    "$$\n",
    "w' \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)\n",
    "$$\n",
    "\n",
    "În practică, implementarea presupune înmulțirea valorilor extrase din distribuția Gaussiana $\\mathcal{N}\\left(0, 1 \\right)$ cu factorul de scalare astfel:\n",
    "\n",
    "$$\n",
    "w' = w \\times \\sqrt{\\frac{2}{n_{in}}}\n",
    "$$\n",
    "\n",
    "[1] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" Proceedings of the IEEE international conference on computer vision. 2015.\n",
    "\n",
    "**Ex1.Implementarea unei rețele neuronale cu 3 straturi.** Implementați algoritmul de propagare înainte pentru modelul prezentat în Figura 4 și folosiți inițializarea He pentru ponderi $(W_1, W_2, W_3)$ și inițializarea cu 0 pentru deplasamente $(b_1, b_2, b_3)$. Pentru un singur exemplu de antrenare, rulați propagarea înainte și afișați dimensiunile vectorilor de ieșire, a ponderilor și a deplasamentului după fiecare strat.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_1 &= X \\cdot W_1 + b_1, \\quad a_1 = \\text{ReLU}(z_1) \\\\\n",
    "z_2 &= a_1 \\cdot W_2 + b_2, \\quad a_2 = \\text{ReLU}(z_2) \\\\\n",
    "z_3 &= a_2 \\cdot W_3 + b_3, \\quad a_3 = \\text{Sigmoid}(z_3)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> Realizați o implementare generală care să poate fi folosită și pentru un X cu mai multe exemple de antrenare. <br>\n",
    "> Vectorizați operațiile de înmulțire. <br>\n",
    "> Pentru generarea unor valori dintr-o distribuție Gaussiană consultați modulul `np.random`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fbc2199-b952-4fcd-8ac1-724761145255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z1': array([[ 1.67851940e-01,  6.89818218e-01, -2.29240188e-01, ...,\n",
      "        -6.23244375e-01,  6.60134440e-01, -8.62512538e-01],\n",
      "       [ 1.55793938e-01,  6.13438360e-01,  2.02289190e-02, ...,\n",
      "        -8.69945487e-01,  1.79566025e-01, -4.40699769e-01],\n",
      "       [-2.54038828e-01,  7.96729073e-01,  2.00334273e-01, ...,\n",
      "        -3.14200749e-01,  6.98301190e-01, -4.44328439e-01],\n",
      "       ...,\n",
      "       [ 2.17814799e-01, -3.81904682e-04, -1.82289141e-02, ...,\n",
      "        -1.22047109e-01, -3.09061264e-02,  4.91933395e-02],\n",
      "       [ 1.32941286e-01,  2.12840145e-02,  5.52955746e-02, ...,\n",
      "         3.02485874e-01, -2.82780488e-02, -4.41235098e-01],\n",
      "       [ 1.03515538e-01, -2.23922944e-01, -3.83666296e-02, ...,\n",
      "        -6.19863795e-01,  4.85505042e-01, -8.14334229e-02]],\n",
      "      shape=(12665, 25)), 'a1': array([[0.16785194, 0.68981822, 0.        , ..., 0.        , 0.66013444,\n",
      "        0.        ],\n",
      "       [0.15579394, 0.61343836, 0.02022892, ..., 0.        , 0.17956603,\n",
      "        0.        ],\n",
      "       [0.        , 0.79672907, 0.20033427, ..., 0.        , 0.69830119,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.2178148 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.04919334],\n",
      "       [0.13294129, 0.02128401, 0.05529557, ..., 0.30248587, 0.        ,\n",
      "        0.        ],\n",
      "       [0.10351554, 0.        , 0.        , ..., 0.        , 0.48550504,\n",
      "        0.        ]], shape=(12665, 25)), 'z2': array([[-0.7205732 ,  0.6125442 , -0.01102831, ...,  0.4318127 ,\n",
      "         0.72662291,  0.44220289],\n",
      "       [-0.52679019,  0.97363868, -0.01897491, ...,  0.00441156,\n",
      "         0.64848186,  0.31103818],\n",
      "       [-0.63326242,  0.46710114, -0.00898255, ...,  0.33834694,\n",
      "         0.64923425,  0.89995128],\n",
      "       ...,\n",
      "       [ 0.15474144, -0.0856519 , -0.10129606, ...,  0.06211447,\n",
      "        -0.08678122, -0.26405045],\n",
      "       [-0.04305431, -0.16008698, -0.31644199, ...,  0.44919695,\n",
      "        -0.03436501, -0.13228756],\n",
      "       [-0.07150188, -0.28209722, -0.4359572 , ...,  0.43679236,\n",
      "         0.29823396, -0.12247349]], shape=(12665, 15)), 'a2': array([[0.        , 0.6125442 , 0.        , ..., 0.4318127 , 0.72662291,\n",
      "        0.44220289],\n",
      "       [0.        , 0.97363868, 0.        , ..., 0.00441156, 0.64848186,\n",
      "        0.31103818],\n",
      "       [0.        , 0.46710114, 0.        , ..., 0.33834694, 0.64923425,\n",
      "        0.89995128],\n",
      "       ...,\n",
      "       [0.15474144, 0.        , 0.        , ..., 0.06211447, 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 0.        , ..., 0.44919695, 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 0.        , ..., 0.43679236, 0.29823396,\n",
      "        0.        ]], shape=(12665, 15)), 'z3': array([[ 0.09186805],\n",
      "       [-0.13674995],\n",
      "       [-0.13698029],\n",
      "       ...,\n",
      "       [-0.09175084],\n",
      "       [-0.12588583],\n",
      "       [ 0.28586783]], shape=(12665, 1)), 'a3': array([[0.52295087],\n",
      "       [0.46586569],\n",
      "       [0.46580837],\n",
      "       ...,\n",
      "       [0.47707837],\n",
      "       [0.46857004],\n",
      "       [0.57098421]], shape=(12665, 1))}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "W1 = np.random.randn(784, 25) * np.sqrt(2 / 784)\n",
    "b1 = np.zeros((1, 25))\n",
    "\n",
    "W2 = np.random.randn(25, 15) * np.sqrt(2 / 25)\n",
    "b2 = np.zeros((1, 15))\n",
    "\n",
    "W3 = np.random.randn(15, 1) * np.sqrt(2 / 15)\n",
    "b3 = np.zeros((1, 1))\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"W1\": W1, \"b1\": b1,\n",
    "    \"W2\": W2, \"b2\": b2,\n",
    "    \"W3\": W3, \"b3\": b3\n",
    "}\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Computes the ReLU (Rectified Linear Unit) activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Input array or scalar (can be a numpy array, list, or single numeric value).\n",
    "\n",
    "    Returns:\n",
    "    - result: The ReLU of the input, element-wise if `x` is an array (same shape as `x`).\n",
    "    \"\"\"\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def forward(X, params):\n",
    "    \"\"\"\n",
    "    Performs the forward pass of the neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data (shape: (m, input_size)).\n",
    "    - params: Dictionary containing weights and biases.\n",
    "\n",
    "    Returns:\n",
    "    - activations: Dictionary containing intermediate and final outputs.\n",
    "    \"\"\"\n",
    "    W1, b1 = params[\"W1\"], params[\"b1\"]\n",
    "    W2, b2 = params[\"W2\"], params[\"b2\"]\n",
    "    W3, b3 = params[\"W3\"], params[\"b3\"]\n",
    "    \n",
    "    z1=np.dot(X,W1)+b1\n",
    "    a1=relu(z1)\n",
    "\n",
    "    z2=np.dot(a1,W2)+b2\n",
    "    a2=relu(z2)\n",
    "\n",
    "    z3=np.dot(a2,W3)+b3\n",
    "    a3=sigmoid(z3)\n",
    "\n",
    "    activations= {\n",
    "        \"z1\": z1, \"a1\": a1,\n",
    "        \"z2\": z2, \"a2\": a2,\n",
    "        \"z3\": z3, \"a3\": a3\n",
    "    }\n",
    "    return activations\n",
    "A=forward(X_train,params)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b60fc-9a20-49c2-8acb-378b5c964d5b",
   "metadata": {},
   "source": [
    "**Ex2. Implementați funcția de pierdere binary cross-entropy loss.**\n",
    "\n",
    "$$\n",
    "\\text{BCELoss} = -\\frac{1}{M} \\sum_{i=1}^{M} \\left( y_i \\times \\log(\\hat{y}_i) + (1 - y_i) \\times \\log(1 - \\hat{y}_i) \\right)\n",
    "$$\n",
    "\n",
    "- unde M reprezintă numărul de exemple \n",
    "- $y_i$ este eticheta reală (0 sau 1)\n",
    "- $\\hat y_i$ este probabilitatea prezisă de model pentru clasa pozitivă (1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75168065-a68c-4f8a-bde6-bb93bbab285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy loss for a batch of predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels (shape: (m, 1) or (m,), where m is the batch size). \n",
    "              Values should be binary (0 or 1).\n",
    "    - y_pred: Predicted probabilities (shape: (m, 1) or (m,)). \n",
    "              Values should be in the range [0, 1], representing the model's confidence (probability)\n",
    "              in predicting the positive class.\n",
    "\n",
    "    Returns:\n",
    "    - loss: The average binary cross-entropy loss for the batch (scalar value).\n",
    "    \"\"\"\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f5e2b-cb85-4d9c-8022-8ec1bc0c71c1",
   "metadata": {},
   "source": [
    "**Ex3. Calculați gradientul funcției de pierdere BCELOSS în funcție de $\\hat{y}$.** Gradientul $\\nabla_\\hat{y} \\text{BCELoss}$ va fi un vector care conține derivatele parțiale ale funcției de pierdere în funcție de fiecare $\\hat{y}_i$ și anume $[\\frac{\\partial \\text{BCELoss}}{\\partial \\hat{y}_0}, \\frac{\\partial \\text{BCELoss}}{\\partial \\hat{y}_1}, ...]$ \n",
    "Atât y, cât și $\\hat{y}$ sunt vectori. Gradientul $\\nabla_\\hat{y} \\text{BCELoss}$ are aceeași dimensiune cu $\\hat{y}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_i}\n",
    "= \\frac{1}{m} \\left( \\frac{\\hat{y}_i - y_i}{\\hat{y}_i (1 - \\hat{y}_i)} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c525e53-0c18-4c84-bc8e-eb72c71cf3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6871007803037085)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy loss for a batch of predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels (shape: (m, 1) or (m,), where m is the batch size). \n",
    "              Values should be binary (0 or 1).\n",
    "    - y_pred: Predicted probabilities (shape: (m, 1) or (m,)). \n",
    "              Values should be in the range [0, 1], representing the model's confidence (probability)\n",
    "              in predicting the positive class.\n",
    "\n",
    "    Returns:\n",
    "    - loss: The average binary cross-entropy loss for the batch (scalar value).\n",
    "    \"\"\"\n",
    "    m=y_true.shape[0]\n",
    "    return -np.sum(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))/m\n",
    "a3=A[\"a3\"]\n",
    "binary_cross_entropy(y_train,a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bacdea5f-61b1-4ce1-b923-acffa55db92c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BCE_loss_derivative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m y = case[\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m expected_derivative = case[\u001b[33m\"\u001b[39m\u001b[33mexpected_derivative\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m computed_derivative = \u001b[43mBCE_loss_derivative\u001b[49m(y, y_hat)\n\u001b[32m     14\u001b[39m matched = np.allclose(computed_derivative, expected_derivative, atol=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest case: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'BCE_loss_derivative' is not defined"
     ]
    }
   ],
   "source": [
    "### RUN TESTS\n",
    "\n",
    "test_cases = [\n",
    "    {\"y_hat\": np.array([0.8, 0.1, 0.5]), \"y\": np.array([1, 0, 1]), \"expected_derivative\": np.array([-0.41666667, 0.37037037, -0.66666667])},\n",
    "    {\"y_hat\": np.array([[0.4], [0.8], [0.2]]), \"y\": np.array([[0], [1], [0]]), \"expected_derivative\": np.array([[0.55555556],[-0.41666667],[ 0.41666667]])}]\n",
    "\n",
    "for i, case in enumerate(test_cases, start=1):\n",
    "    y_hat = case[\"y_hat\"]\n",
    "    y = case[\"y\"]\n",
    "    expected_derivative = case[\"expected_derivative\"]\n",
    "    \n",
    "    computed_derivative = BCE_loss_derivative(y, y_hat)\n",
    "\n",
    "    matched = np.allclose(computed_derivative, expected_derivative, atol=1e-4)\n",
    "    \n",
    "    print(f\"Test case: {i}\")\n",
    "    print(\"Test:\", \"PASS\" if matched else \"FAIL\")\n",
    "    \n",
    "    if not matched:\n",
    "        print(f\"Expected Derivative: {expected_derivative}\")\n",
    "        print(f\"Computed Derivative: {computed_derivative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c19d2b-06f9-4b87-b130-f133c66995a8",
   "metadata": {},
   "source": [
    "**Ex4. Calculați derivata funcției sigmoid $\\sigma(x)$ față de un input x.**\n",
    "\n",
    "$$ \\sigma'(x) = \\sigma(x) \\times (1 - \\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a4639d3-b43a-40a5-a2c7-701410e614be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Input array or scalar (can be a numpy array, list, or single numeric value).\n",
    "         Represents the input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - result: The derivative of the sigmoid function, element-wise if `x` is an array.\n",
    "              It has the same shape as `x`.\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5057de-883b-4c18-98d6-99d26f048fb5",
   "metadata": {},
   "source": [
    "În continuare vom calcula gradientul funcției de pierdere față de fiecare parametru al modelului prin metoda propagării înapoi. \n",
    "\n",
    "**Exemplu simplu al propagării inapoi.**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/computational_graph.png\" alt=\"image\" width=\"800\"/>\n",
    "</p> \n",
    "\n",
    "\n",
    "Pentru modelul MLP, vom începe prin calcularea derivatei funcției de pierdere față de $a_3$ (probabilitatea prezisă notată și cu $\\hat{y}$), după care vom folosi regula lanțului și propagarea înapoi pentru a calcula toate derivatele:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{i,k}} = \\frac{\\partial L}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial w_{i,k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba05b5-a4c7-4e27-a54c-bc9b1a5310b5",
   "metadata": {},
   "source": [
    "**Ex5. Calculați gradientul funcției de pierdere BCE Loss pentru ultimul strat al modelului** pentru un lot (batch) de exemple X de dimensiune (m, input_size). Se vor calcula  $\\frac{\\partial L}{\\partial W_3}$ și  $\\frac{\\partial L}{\\partial b_3}$.\n",
    "\n",
    "\n",
    "În propagarea înapoi, întâlnim 2 tipuri de înmulțiri:\n",
    "- dacă în propagarea înainte avem o **transformare liniară**, adică o înmulțire matriceală sau un produs scalar $z = x \\cdot W + b$, atunci în propagarea înapoi se folosește tot produs scalar sau înmulțire matriceală pentru a propaga gradienții.\n",
    "\n",
    "Gradientul lui W se calculează cu regula lanțului în felul următor:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{L}}{\\partial W} = x^T \\cdot \\frac{\\partial \\text{L}}{\\partial z}\n",
    "$$\n",
    "\n",
    "unde $\\frac{\\partial \\text{L}}{\\partial z}$ reprezintă gradientul venit din stratul anterior, adică din \"upstream\", iar gradientul lui x se calculează ca:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{L}}{\\partial x} = \\frac{\\partial \\text{L}}{\\partial z} \\cdot W^T \n",
    "$$\n",
    "\n",
    "- dacă în propagarea înainte avem o operație care se aplică **element cu element** asupra unei matrici sau unui vector (precum ReLU, Sigmoid, înmulțire cu un scalar), atunci vom folosi înmulțirea element cu element în propagarea înapoi\n",
    "\n",
    "\n",
    "Pentru ultimul strat care aplică o transformare liniară $z_3 = a_2 \\cdot W_3 + b_3$, unde dimensiunile sunt $a_2(m,p)$, $W_3(p,1)$, $b_3(1, 1)$, $z_3(m, 1)$, $a_3(m, 1)$, iar m este numărul de exemple din lot, regula lanțului se aplică astfel:\n",
    "\n",
    "**Propagarea înainte $\\rightarrow$**:\n",
    "\n",
    "$$\n",
    "z_3 = a_2 \\cdot W_3 + b_3\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_3 = \\text{Sigmoid}(z_3)\n",
    "$$\n",
    "\n",
    "**Propagarea înapoi $\\leftarrow$**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial z_3} = \\frac{\\partial \\text{BCELoss}}{\\partial a_3} \\times \\frac{\\partial a_3}{\\partial z_3} \\text{(înmulțire element cu element)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial W_3} = \\frac{1}{m}a_2^T \\cdot \\frac{\\partial \\text{BCELoss}}{\\partial z_3}  \\text{(înmulțire de matrici)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial b_3} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\frac{\\partial  \\text{BCELoss}}{\\partial z_3^i} \\text{(media pe coloane)}\n",
    "$$ \n",
    "\n",
    "> *Hint*: Apelați funcția de propagare pentru batch-ul de exemple de input o singură dată. <br>\n",
    "> Dimensiunea gradientului în funcție de un parametru este întotdeauna egală cu dimensiunea parametrului <br>\n",
    "> $a_3$ reprezintă de fapt $\\hat{y}$ folosit în calculul `BCELoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e9306f3-e202-4d08-9bea-6253433741ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def backward_third_layer(y: np.ndarray, \n",
    "                        activations: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes vectorized gradients of the last layer for backpropagation over a batch.\n",
    "\n",
    "    Parameters:\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - activations (Dict[str, np.ndarray]): Dictionary containing activations and pre-activations of the network.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        Gradients computed for the last layer:\n",
    "        - dz3 : Gradient of the loss w.r.t. the pre-activation (z3), shape (m, 1).\n",
    "        - dW3  : Gradient of the loss w.r.t. the weights (W3), shape (p, 1).\n",
    "        - db3  : Gradient of the loss w.r.t. the biases (b3), shape (1, 1).\n",
    "\n",
    "    \"\"\"\n",
    "    a2 = activations[\"a2\"]   \n",
    "    a3 = activations[\"a3\"]   \n",
    "    z3 = activations[\"z3\"] \n",
    "    m = y.shape[0]\n",
    "    dz3 = BCE_loss_derivative(y, a3) * sigmoid_derivative(z3)\n",
    "    dW3 = a2.T.dot(dz3) / m\n",
    "    db3 = np.mean(dz3)\n",
    "    return dz3, dW3, db3\n",
    "\n",
    "def forward_backward_third_layer(X: np.ndarray, \n",
    "                                y: np.ndarray, \n",
    "                                params: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "     Performs a full forward pass for the entire network to compute the intermediate activations \n",
    "    and backward pass for the last layer for a single example.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features for the batch, shape (m, p).\n",
    "                      `m` is the number of examples, `p` is the number of features.\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W3, b3).\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[np.ndarray, np.ndarray]: Gradients (dz3, dW3, db3)\n",
    "    \"\"\"\n",
    "    activations = forward(X, params)\n",
    "    dz3, dW3, db3 = backward_third_layer(y, activations)\n",
    "    return dz3, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e5dd31f-7660-4791-af47-a0f53ba7749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TESTS\n",
    "\n",
    "def test_backward(forward_backward_func, params, X_batch, y_batch, expected_dW, expected_db):\n",
    "     \n",
    "    # Run forward and backward pass\n",
    "    W1, b1 = params[\"W1\"], params[\"b1\"]\n",
    "    W2, b2 = params[\"W2\"], params[\"b2\"]\n",
    "    W3, b3 = params[\"W3\"], params[\"b3\"]\n",
    "    _, dW, db = forward_backward_func(X_batch, y_batch, params)\n",
    "      \n",
    "    dW_pass = np.allclose(dW, expected_dW, atol=1e-4)\n",
    "    db_pass = np.allclose(db, expected_db, atol=1e-4)\n",
    "    \n",
    "    print(\"Test Results:\")\n",
    "    print(\"dW Test:\", \"PASS\" if dW_pass else \"FAIL\")\n",
    "    print(\"db Test:\", \"PASS\" if db_pass else \"FAIL\")\n",
    "    \n",
    "    if not dW_pass:\n",
    "        print(\"\\nMismatch in dW:\")\n",
    "        print(\"Computed dW:\")\n",
    "        print(dW)\n",
    "        print(\"Expected dW:\")\n",
    "        print(expected_dW)\n",
    "    \n",
    "    if not db_pass:\n",
    "        print(\"\\nMismatch in db:\")\n",
    "        print(\"Computed db:\")\n",
    "        print(db)\n",
    "        print(\"Expected db:\")\n",
    "        print(expected_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83a3eeb6-8531-4bfb-b38a-c66317f94df7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BCE_loss_derivative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m expected_dW3 = np.array([[\u001b[32m0.00510939\u001b[39m],[\u001b[32m0.\u001b[39m]])\n\u001b[32m     24\u001b[39m expected_db3 = np.array([[\u001b[32m0.08862105\u001b[39m]])\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mtest_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_backward_third_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_dW3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_db3\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtest_backward\u001b[39m\u001b[34m(forward_backward_func, params, X_batch, y_batch, expected_dW, expected_db)\u001b[39m\n\u001b[32m      7\u001b[39m W2, b2 = params[\u001b[33m\"\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m\"\u001b[39m], params[\u001b[33m\"\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m W3, b3 = params[\u001b[33m\"\u001b[39m\u001b[33mW3\u001b[39m\u001b[33m\"\u001b[39m], params[\u001b[33m\"\u001b[39m\u001b[33mb3\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m _, dW, db = \u001b[43mforward_backward_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m dW_pass = np.allclose(dW, expected_dW, atol=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     12\u001b[39m db_pass = np.allclose(db, expected_db, atol=\u001b[32m1e-4\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mforward_backward_third_layer\u001b[39m\u001b[34m(X, y, params)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m Performs a full forward pass for the entire network to compute the intermediate activations \u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03mand backward pass for the last layer for a single example.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m \u001b[33;03m- Tuple[np.ndarray, np.ndarray]: Gradients (dz3, dW3, db3)\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     49\u001b[39m activations = forward(X, params)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m dz3, dW3, db3 = \u001b[43mbackward_third_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dz3, dW3, db3\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mbackward_third_layer\u001b[39m\u001b[34m(y, activations)\u001b[39m\n\u001b[32m     25\u001b[39m z3 = activations[\u001b[33m\"\u001b[39m\u001b[33mz3\u001b[39m\u001b[33m\"\u001b[39m] \n\u001b[32m     26\u001b[39m m = y.shape[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m dz3 = \u001b[43mBCE_loss_derivative\u001b[49m(y, a3) * sigmoid_derivative(z3)\n\u001b[32m     28\u001b[39m dW3 = a2.T.dot(dz3) / m\n\u001b[32m     29\u001b[39m db3 = np.mean(dz3)\n",
      "\u001b[31mNameError\u001b[39m: name 'BCE_loss_derivative' is not defined"
     ]
    }
   ],
   "source": [
    "### RUN TESTS\n",
    "\n",
    "W1 = np.array([[0.1, -0.05, 0.02],\n",
    "               [0.03, 0.07, -0.02]])\n",
    "b1 = np.array([[0.01, 0.02, -0.03]])\n",
    "W2 = np.array([[0.1, -0.1],\n",
    "               [0.05, 0.02],\n",
    "               [-0.03, 0.06]])\n",
    "b2 = np.array([[0.03, -0.02]])\n",
    "\n",
    "W3 = np.array([[5.0], [-3.0]])  \n",
    "b3 = np.array([[0.5]])          \n",
    "\n",
    "params = {\n",
    "    \"W1\": W1, \"b1\": b1,\n",
    "    \"W2\": W2, \"b2\": b2,\n",
    "    \"W3\": W3, \"b3\": b3\n",
    "}\n",
    "\n",
    "X_t = np.array([[1.5, -0.3], [0.7, 2.2]])  \n",
    "y_t = np.array([[1], [0]]) \n",
    "\n",
    "expected_dW3 = np.array([[0.00510939],[0.]])\n",
    "expected_db3 = np.array([[0.08862105]])\n",
    "\n",
    "test_backward(forward_backward_third_layer, params, X_t, y_t, expected_dW3, expected_db3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95daccf2-c383-4d5a-b6ce-c4115c7c8fe3",
   "metadata": {},
   "source": [
    "**Ex6. Calculați gradientul funcției de pierdere pentru al doilea strat al modelului pentru un lot de exemple.** Pentru a calcula $\\frac{\\partial L}{\\partial W_2}$ și  $\\frac{\\partial L}{\\partial b_2}$, folosiți gradienții calculați până acum și backpropagation.\n",
    "\n",
    "**Propagarea înainte $\\rightarrow$:**\n",
    "$$\n",
    "z_2 = a_1 \\cdot W_2 + b_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_2 = \\text{ReLU}(z_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_3 = a_2 \\cdot W_3 + b_3\n",
    "$$\n",
    "\n",
    "**Propagarea înapoi $\\leftarrow$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial a_2} = \\frac{\\partial \\text{BCELoss}}{\\partial z_3} \\cdot W_3^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial z_2} = \\frac{\\partial \\text{BCELoss}}{\\partial a_2} \\times ReLU'(z_2)\n",
    "$$\n",
    "\n",
    "unde \n",
    "\n",
    "$$\n",
    "\\text{ReLU'}(x) =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } x > 0 \\\\\n",
    "0, & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial W_2} = \\frac{1}{m}a_1^T \\cdot \\frac{\\partial \\text{BCELoss}}{\\partial z_2}  \\text{(înmulțire de matrici)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial b_2} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\frac{\\partial \\text{BCELoss}}{\\partial z_2^i} \\text{(media pe coloane)}\n",
    "$$ \n",
    "\n",
    "> *Hint*: Refolosiți funcția `backward_third_layer` pentru a calcula $\\frac{\\text{BCELoss}}{\\partial z_3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e401fb9-65be-4186-9d65-03648fe11734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return (x>0).astype(float)\n",
    "\n",
    "def backward_second_layer(y: np.ndarray, \n",
    "                          params: Dict[str, np.ndarray], \n",
    "                          activations: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes vectorized gradients of the second layer for backpropagation over a batch.\n",
    "\n",
    "    Parameters:\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W2, b2, W3, b3).\n",
    "    - activations (Dict[str, np.ndarray]): Dictionary containing activations and pre-activations of the network.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        Gradients computed for the last layer:\n",
    "        - dz2 : Gradient of the loss w.r.t. the pre-activation (z2)\n",
    "        - dW2  : Gradient of the loss w.r.t. the weights (W2)\n",
    "        - db2  : Gradient of the loss w.r.t. the biases (b2)\n",
    "\n",
    "    \"\"\"\n",
    "    dz3, _, _ = backward_third_layer(y, activations)\n",
    "\n",
    "    a1 = activations[\"a1\"]\n",
    "    z2 = activations[\"z2\"]\n",
    "    W3 = params[\"W3\"]\n",
    "\n",
    "    da2 = dz3 @ W3.T #loss a2\n",
    "    dz2 = da2 * relu_derivative(z2)\n",
    "\n",
    "    m = y.shape[0]\n",
    "    dW2 = (a1.T @ dz2) / m\n",
    "    db2 = dz2.sum(axis=0, keepdims=True) / m\n",
    "\n",
    "    return dz2, dW2, db2\n",
    "\n",
    "def forward_backward_second_layer(X: np.ndarray, \n",
    "                                  y: np.ndarray, \n",
    "                                  params: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Performs a full forward pass for the entire network to compute the intermediate activations \n",
    "    and backward pass for the second layer using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features for the batch, shape (m, n).\n",
    "                      `m` is the number of examples, `n` is the number of features.\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W2, b2, W3, b3).\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[np.ndarray, np.ndarray]: Gradients (dz2, dW2, db2)\n",
    "    \"\"\"\n",
    "    activations = forward(X, params)\n",
    "    dz2, dW2, db2 = backward_second_layer(y, params, activations)\n",
    "\n",
    "    return dz2, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad41616a-d6cb-47b5-a0a4-e5be67a3cb82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BCE_loss_derivative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m expected_dW2 = np.array([[ \u001b[32m0.10924257\u001b[39m, -\u001b[32m0.10250817\u001b[39m],[ \u001b[32m0.16265566\u001b[39m, -\u001b[32m0.09759339\u001b[39m], [-\u001b[32m0.00244786\u001b[39m, \u001b[32m0\u001b[39m]])\n\u001b[32m     24\u001b[39m expected_db2 = np.array([[\u001b[32m0.76220853\u001b[39m, -\u001b[32m0.70211075\u001b[39m]])\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mtest_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_backward_second_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_dW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_db2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtest_backward\u001b[39m\u001b[34m(forward_backward_func, params, X_batch, y_batch, expected_dW, expected_db)\u001b[39m\n\u001b[32m      7\u001b[39m W2, b2 = params[\u001b[33m\"\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m\"\u001b[39m], params[\u001b[33m\"\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m W3, b3 = params[\u001b[33m\"\u001b[39m\u001b[33mW3\u001b[39m\u001b[33m\"\u001b[39m], params[\u001b[33m\"\u001b[39m\u001b[33mb3\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m _, dW, db = \u001b[43mforward_backward_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m dW_pass = np.allclose(dW, expected_dW, atol=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     12\u001b[39m db_pass = np.allclose(db, expected_db, atol=\u001b[32m1e-4\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mforward_backward_second_layer\u001b[39m\u001b[34m(X, y, params)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03mPerforms a full forward pass for the entire network to compute the intermediate activations \u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03mand backward pass for the second layer using vectorized operations.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m \u001b[33;03m- Tuple[np.ndarray, np.ndarray]: Gradients (dz2, dW2, db2)\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m activations = forward(X, params)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m dz2, dW2, db2 = \u001b[43mbackward_second_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dz2, dW2, db2\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mbackward_second_layer\u001b[39m\u001b[34m(y, params, activations)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward_second_layer\u001b[39m(y: np.ndarray, \n\u001b[32m      5\u001b[39m                           params: Dict[\u001b[38;5;28mstr\u001b[39m, np.ndarray], \n\u001b[32m      6\u001b[39m                           activations: Dict[\u001b[38;5;28mstr\u001b[39m, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n\u001b[32m      7\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    Computes vectorized gradients of the second layer for backpropagation over a batch.\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     dz3, _, _ = \u001b[43mbackward_third_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     a1 = activations[\u001b[33m\"\u001b[39m\u001b[33ma1\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     27\u001b[39m     z2 = activations[\u001b[33m\"\u001b[39m\u001b[33mz2\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mbackward_third_layer\u001b[39m\u001b[34m(y, activations)\u001b[39m\n\u001b[32m     25\u001b[39m z3 = activations[\u001b[33m\"\u001b[39m\u001b[33mz3\u001b[39m\u001b[33m\"\u001b[39m] \n\u001b[32m     26\u001b[39m m = y.shape[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m dz3 = \u001b[43mBCE_loss_derivative\u001b[49m(y, a3) * sigmoid_derivative(z3)\n\u001b[32m     28\u001b[39m dW3 = a2.T.dot(dz3) / m\n\u001b[32m     29\u001b[39m db3 = np.mean(dz3)\n",
      "\u001b[31mNameError\u001b[39m: name 'BCE_loss_derivative' is not defined"
     ]
    }
   ],
   "source": [
    "### RUN TESTS\n",
    "\n",
    "W1 = np.array([[0.1, -0.05, 0.02],\n",
    "               [0.03, 0.07, -0.02]])\n",
    "b1 = np.array([[0.01, 0.02, -0.03]])\n",
    "W2 = np.array([[0.1, 0.1],\n",
    "               [4.0, 2.0],\n",
    "               [-0.03, 0.06]])\n",
    "b2 = np.array([[0.03, -0.02]])\n",
    "\n",
    "W3 = np.array([[5.0], [-3.0]])  \n",
    "b3 = np.array([[0.5]])          \n",
    "\n",
    "params = {\n",
    "    \"W1\": W1, \"b1\": b1,\n",
    "    \"W2\": W2, \"b2\": b2,\n",
    "    \"W3\": W3, \"b3\": b3\n",
    "}\n",
    "\n",
    "X_t = np.array([[1.5, -0.3], [0.7, 2.2]])  \n",
    "y_t = np.array([[1], [0]]) \n",
    "\n",
    "expected_dW2 = np.array([[ 0.10924257, -0.10250817],[ 0.16265566, -0.09759339], [-0.00244786, 0]])\n",
    "expected_db2 = np.array([[0.76220853, -0.70211075]])\n",
    "\n",
    "test_backward(forward_backward_second_layer, params, X_t, y_t, expected_dW2, expected_db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937fcba7-b71e-40d7-a164-942eb3b9509f",
   "metadata": {},
   "source": [
    "**Ex7. Calculați gradientul funcției de pierdere pentru primul strat al modelului pentru un lot de exemple.** Pentru a calcula $\\frac{\\partial L}{\\partial W_1}$ și  $\\frac{\\partial L}{\\partial b_1}$, folosiți gradienții calculați până acum și backpropagation. \n",
    "\n",
    "\n",
    "**Propagarea înainte $\\rightarrow$**\n",
    "$$\n",
    "z_1 = X \\cdot W_1 + b_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_1 = \\text{ReLU}(z_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_2 = a_1 \\cdot W_2 + b_2\n",
    "$$\n",
    "\n",
    "\n",
    "**Propagarea înapoi $\\leftarrow$**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial a_1} = \\frac{\\partial \\text{BCELoss}}{\\partial z_2} \\cdot W_2^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial z_1} = \\frac{\\partial \\text{BCELoss}}{\\partial a_1} \\times ReLU'(z_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial W_1} = \\frac{1}{m}X^T \\cdot \\frac{\\partial \\text{BCELoss}}{\\partial z_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial b_1} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\frac{\\partial \\text{BCELoss}}{\\partial z_1^i}\n",
    "$$ \n",
    "\n",
    "> *Hint*: Refolosiți funcția `backward_second_layer` pentru a calcula $\\frac{\\text{BCELoss}}{\\partial z_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0255963-970d-4527-9dc9-51c1ec013216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_first_layer(X: np.ndarray, \n",
    "                                          y: np.ndarray, \n",
    "                                          params: Dict[str, np.ndarray], \n",
    "                                          activations: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes vectorized gradients of the first layer for backpropagation over a batch.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features for the batch, shape (m, n).\n",
    "                      `m` is the number of examples, `n` is the number of input features.\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W1, b1, W2, b2, W3, b3).\n",
    "    - activations (Dict[str, np.ndarray]): Dictionary containing activations and pre-activations of the network.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        Gradients computed for the last layer:\n",
    "        - dz1 : Gradient of the loss w.r.t. the pre-activation (z1)\n",
    "        - dW1  : Gradient of the loss w.r.t. the weights (W1)\n",
    "        - db1  : Gradient of the loss w.r.t. the biases (b1)\n",
    "    \"\"\"\n",
    "        \n",
    "    #m = batch size\n",
    "    #n = nr de features \n",
    "    \n",
    "    W2 = activations[\"W2\"]\n",
    "    z1 = activations[\"z1\"]\n",
    "    m = X.shape[0]\n",
    "\n",
    "    da1 = dz2 @ W2.T\n",
    "    dz1 = da1 * relu_derivative(z1)\n",
    "\n",
    "    dW1 = (X.T @ dz1) / m\n",
    "    db1 = dz1.sum(axis=0, keepdims=True) / m\n",
    "\n",
    "    return dz1, dW1, db1\n",
    "\n",
    "def forward_backward_first_layer(X: np.ndarray, \n",
    "                        y: np.ndarray, \n",
    "                        params: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Performs a full forward pass for the entire network to compute the intermediate activations \n",
    "    and backward pass for the first layer using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features for the batch, shape (m, n).\n",
    "                      `m` is the number of examples, `n` is the number of input features.\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W1, b1, W2, b2, W3, b3).\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[np.ndarray, np.ndarray]: Gradients (dz1, dW1, db1) for the first layer weights and biases.\n",
    "    \"\"\"\n",
    "    activations = forward(X, params)\n",
    "\n",
    "    dz3, dW3, db3 = backward_third_layer(y, activations)\n",
    "    dz2, dW2, db2 = backward_second_layer(y, params, activations)\n",
    "    dz1, dW1, db1 = backward_first_layer(X, params, activations, dz2)\n",
    "\n",
    "    return dz1, dW1, db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c69d9bd4-6ed2-4aea-8aec-677b2751818f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BCE_loss_derivative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     23\u001b[39m expected_dW1 = np.array([[-\u001b[32m2.60110482e-02\u001b[39m, \u001b[32m2.45\u001b[39m, \u001b[32m1.83033145e-02\u001b[39m],\n\u001b[32m     24\u001b[39m  [ \u001b[32m1.22202210e-01\u001b[39m, \u001b[32m7.7\u001b[39m, -\u001b[32m3.66066289e-03\u001b[39m]])\n\u001b[32m     25\u001b[39m expected_db1 = np.array([[ \u001b[32m0.00932597\u001b[39m, \u001b[32m3.5\u001b[39m, \u001b[32m0.01220221\u001b[39m]])\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtest_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_backward_first_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_dW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_db1\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtest_backward\u001b[39m\u001b[34m(forward_backward_func, params, X_batch, y_batch, expected_dW, expected_db)\u001b[39m\n\u001b[32m      7\u001b[39m W2, b2 = params[\u001b[33m\"\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m\"\u001b[39m], params[\u001b[33m\"\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m W3, b3 = params[\u001b[33m\"\u001b[39m\u001b[33mW3\u001b[39m\u001b[33m\"\u001b[39m], params[\u001b[33m\"\u001b[39m\u001b[33mb3\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m _, dW, db = \u001b[43mforward_backward_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m dW_pass = np.allclose(dW, expected_dW, atol=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     12\u001b[39m db_pass = np.allclose(db, expected_db, atol=\u001b[32m1e-4\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mforward_backward_first_layer\u001b[39m\u001b[34m(X, y, params)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03mPerforms a full forward pass for the entire network to compute the intermediate activations \u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03mand backward pass for the first layer using vectorized operations.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m \u001b[33;03m- Tuple[np.ndarray, np.ndarray]: Gradients (dz1, dW1, db1) for the first layer weights and biases.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m activations = forward(X, params)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m dz3, dW3, db3 = \u001b[43mbackward_third_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m dz2, dW2, db2 = backward_second_layer(y, params, activations)\n\u001b[32m     60\u001b[39m dz1, dW1, db1 = backward_first_layer(X, params, activations, dz2)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mbackward_third_layer\u001b[39m\u001b[34m(y, activations)\u001b[39m\n\u001b[32m     25\u001b[39m z3 = activations[\u001b[33m\"\u001b[39m\u001b[33mz3\u001b[39m\u001b[33m\"\u001b[39m] \n\u001b[32m     26\u001b[39m m = y.shape[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m dz3 = \u001b[43mBCE_loss_derivative\u001b[49m(y, a3) * sigmoid_derivative(z3)\n\u001b[32m     28\u001b[39m dW3 = a2.T.dot(dz3) / m\n\u001b[32m     29\u001b[39m db3 = np.mean(dz3)\n",
      "\u001b[31mNameError\u001b[39m: name 'BCE_loss_derivative' is not defined"
     ]
    }
   ],
   "source": [
    "### RUN TESTS\n",
    "\n",
    "W1 = np.array([[0.1, -0.05, 0.2],\n",
    "               [0.3, 0.7, -0.02]])\n",
    "b1 = np.array([[0.1, 0.2, -0.3]])\n",
    "W2 = np.array([[0.1, 0.1],\n",
    "               [4.0, 2.0],\n",
    "               [-0.03, 0.06]])\n",
    "b2 = np.array([[0.03, -0.02]])\n",
    "\n",
    "W3 = np.array([[5.0], [-3.0]])  \n",
    "b3 = np.array([[0.5]])          \n",
    "\n",
    "params = {\n",
    "    \"W1\": W1, \"b1\": b1,\n",
    "    \"W2\": W2, \"b2\": b2,\n",
    "    \"W3\": W3, \"b3\": b3\n",
    "}\n",
    "\n",
    "X_t = np.array([[1.5, -0.3], [0.7, 2.2]])  \n",
    "y_t = np.array([[1], [0]]) \n",
    "\n",
    "expected_dW1 = np.array([[-2.60110482e-02, 2.45, 1.83033145e-02],\n",
    " [ 1.22202210e-01, 7.7, -3.66066289e-03]])\n",
    "expected_db1 = np.array([[ 0.00932597, 3.5, 0.01220221]])\n",
    "\n",
    "test_backward(forward_backward_first_layer, params, X_t, y_t, expected_dW1, expected_db1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afcb48-d08b-4633-a4ce-59b0964eabb6",
   "metadata": {},
   "source": [
    "**Ex8. Implementați funcția `backward` care calculează gradienții funcției de pierdere față de toți parametrii modelului ($W_1, W_2, W_3, b_1, b_2, b_3$) și face un update al fiecărui parametru în parte cu gradient descent**:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} \\text{BCELoss}\n",
    "$$\n",
    "\n",
    "- unde $\\alpha$ este rata de învățare\n",
    "- $\\theta$ reprezintă fiecare parametru al modelului ($W_1, W_2, W_3, b_1, b_2, b_3$)\n",
    "- Pentru a calcula gradienții, apelați funcțiile implementate mai sus `backward_third_layer`, `backward_second_layer`, `backward_first_layer` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4967bf3c-4eb8-4bf9-af20-7058875a4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X: np.ndarray, \n",
    "             y: np.ndarray, \n",
    "             params: Dict[str, np.ndarray], \n",
    "             activations: Dict[str, np.ndarray], \n",
    "             learning_rate: float = 0.001) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Performs the backward pass of the neural network and updates the parameters using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features for the batch, shape (m, n).\n",
    "                      `m` is the number of examples, `n` is the number of input features.\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W1, b1, W2, b2, W3, b3).\n",
    "    - activations (Dict[str, np.ndarray]): Dictionary containing activations and pre-activations of the network.\n",
    "    - learning_rate (float, optional): Learning rate for gradient descent. Default is 0.001.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, np.ndarray]: Updated model parameters after gradient descent.\n",
    "    \"\"\"\n",
    "    dz3, dW3, db3 = backward_third_layer(y, activations)\n",
    "    dz2, dW2, db2 = backward_second_layer(y, params, activations)\n",
    "    dz1, dW1, db1 = backward_first_layer(X, y, params, activations)\n",
    "\n",
    "    params[\"W3\"] -= learning_rate * dW3\n",
    "    params[\"b3\"] -= learning_rate * db3\n",
    "\n",
    "    params[\"W2\"] -= learning_rate * dW2\n",
    "    params[\"b2\"] -= learning_rate * db2\n",
    "\n",
    "    params[\"W1\"] -= learning_rate * dW1\n",
    "    params[\"b1\"] -= learning_rate * db1\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf4a77c-4c0f-4b09-8455-2956427a4d45",
   "metadata": {},
   "source": [
    "**Ex9. Implementați funcția de antrenare.** Se va itera setul de date $X_{train}$ pentru un număr fix de epoci. Într-o epocă se vizitează toate exemplele din setul de antrenare. Se va folosi mini-batch gradient descent, unde se vor propaga înainte doar câte un batch de exemple din setul de antrenare întreg. Afișați la fiecare epocă valoarea funcției de pierdere. Afișați graficul funcției de pierdere.\n",
    "\n",
    "### Pseudocod\n",
    "\n",
    "**Input**: \n",
    "- `X_train` (training data), `y_train` (training labels)\n",
    "\n",
    "**Hyperparameters**:\n",
    "- `num_epochs` (number of epochs)\n",
    "-  `batch_size` (mini-batch size)\n",
    "- `learning_rate` (learning rate)\n",
    "\n",
    "**Output**: \n",
    "- Prints average loss per epoch\n",
    "\n",
    "**Initialize Parameters**:\n",
    "   ```plaintext\n",
    "   num_epochs ← 30\n",
    "   batch_size ← 64  # Mini-batch size\n",
    "   learning_rate ← 0.1\n",
    "   num_batches ← floor(X_train.shape[0] / batch_size)  # Number of batches per epoch\n",
    "```\n",
    "\n",
    "**Training Loop**:\n",
    "```plaintext\n",
    "FOR epoch IN range(1, num_epochs + 1):\n",
    "    # Shuffle the data at the start of each epoch\n",
    "    indices ← generate shuffled indices for X_train\n",
    "    X_train ← reorder X_train using indices\n",
    "    y_train ← reorder y_train using indices\n",
    "\n",
    "    epoch_loss ← 0  # Initialize epoch loss\n",
    "\n",
    "    # Loop over mini-batches\n",
    "    FOR i IN range(num_batches):\n",
    "        start ← i * batch_size\n",
    "        end ← start + batch_size\n",
    "        \n",
    "        # Get the mini-batch\n",
    "        X_batch ← X_train[start:end]\n",
    "        y_batch ← y_train[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        output ← forward(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss ← binary_cross_entropy(y_batch, output)\n",
    "        epoch_loss ← epoch_loss + loss\n",
    "        \n",
    "        # Backward pass\n",
    "        backward(X_batch, y_batch, output, learning_rate)\n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    avg_epoch_loss ← epoch_loss / num_batches\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0632bb5-3f46-4aae-8731-760a0deabcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# define again W1, W2, W3, B1, B2, B3 and 'params' as you did in ex. 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3d317-12bb-44d8-ba2f-1203b9e8e68a",
   "metadata": {},
   "source": [
    "**Ex10. Implementați funcția de testare.** Rulați funcția `forward` pe imaginile din setul de test. Pentru a găsi clasa prezisă, se verifică probabilitatea obținută:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\text{if } p(y=1|x; \\theta) < 0.5 \\text{ predict class 0} \\\\\n",
    "\\text{if } p(y=1|x; \\theta) \\geq 0.5 \\text{ predict class 1}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Se va calcula de asemenea acuratețea pe setul de test. Afișați câteva imagini din setul de test și eticheta prezisă.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\n",
    "$$\n",
    "\n",
    "\n",
    "> *Hint*:  $p(y=1|x; \\theta)$ reprezintă $a_3$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3769c92a-3190-4d81-84cd-a2f7c9a2fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcec545-9e69-4a92-9c69-0cff3d83e516",
   "metadata": {},
   "source": [
    "Pentru a întelege mai bine cum funcționează rețelele neuronale și cum învață acestea din date se poate accesa aplicația web [playground.tensorflow.org](https://playground.tensorflow.org/) pentru vizualizarea interactivă a acestora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8a99c-787b-4583-833d-2afc1205af9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26540422-4921-47bc-8f80-8efb170962b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d95612-bfc6-49e3-aa3e-15cb2dd47634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ad121-d08e-4967-bdc0-cdb9ccdade3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
